"""
å‰µå»ºå¹³è¡¡çš„ Fine-tune Dataset
ç­–ç•¥: Under-sampling Head class + Over-sampling Tail class
"""
import os
import shutil
import random
from pathlib import Path
from collections import defaultdict
import yaml

def create_balanced_dataset(
    original_train_img='data/train/images',
    original_train_label='data/train/labels',
    output_dir='data/train_balanced',
    target_samples_per_class=3000,  # â­ æ¯å€‹é¡åˆ¥çš„ç›®æ¨™æ¨£æœ¬æ•¸
):
    """
    å‰µå»ºå¹³è¡¡çš„è¨“ç·´é›†
    
    ç­–ç•¥:
    - Class 0 (14421) â†’ Under-sample åˆ° 3000
    - Class 1 (647)   â†’ Over-sample åˆ° 3000 (è¤‡è£½ ~4.6x)
    - Class 2 (1924)  â†’ Over-sample åˆ° 3000 (è¤‡è£½ ~1.6x)
    - Class 3 (2854)  â†’ Over-sample åˆ° 3000 (è¤‡è£½ ~1.05x)
    """
    
    print("="*80)
    print("å‰µå»ºå¹³è¡¡çš„ Fine-tune Dataset")
    print("="*80)
    
    original_train_img = Path(original_train_img)
    original_train_label = Path(original_train_label)
    output_dir = Path(output_dir)
    
    output_img_dir = output_dir / 'images'
    output_label_dir = output_dir / 'labels'
    
    output_img_dir.mkdir(parents=True, exist_ok=True)
    output_label_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“ è¼¸å…¥:")
    print(f"   åœ–ç‰‡: {original_train_img}")
    print(f"   æ¨™ç±¤: {original_train_label}")
    print(f"\nğŸ“ è¼¸å‡º:")
    print(f"   å¹³è¡¡é›†: {output_dir}")
    print(f"\nâš™ï¸  ç­–ç•¥:")
    print(f"   ç›®æ¨™æ¨£æœ¬æ•¸/é¡åˆ¥: {target_samples_per_class}")
    
    # â­ Step 1: åˆ†ææ¯å¼µåœ–ç‰‡çš„é¡åˆ¥åˆ†ä½ˆ
    print(f"\nğŸ” åˆ†æåŸå§‹è¨“ç·´é›†...")
    
    image_class_map = defaultdict(set)  # {image_name: {class_ids}}
    class_image_map = defaultdict(list)  # {class_id: [image_names]}
    
    for label_file in original_train_label.glob('*.txt'):
        img_name = label_file.stem
        
        with open(label_file, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    class_id = int(parts[0])
                    image_class_map[img_name].add(class_id)
                    class_image_map[class_id].append(img_name)
    
    # çµ±è¨ˆ
    print(f"\nğŸ“Š åŸå§‹åˆ†ä½ˆ (åœ–ç‰‡å±¤ç´š):")
    for class_id in sorted(class_image_map.keys()):
        count = len(set(class_image_map[class_id]))  # å»é‡
        print(f"   Class {class_id}: {count} å¼µåœ–ç‰‡åŒ…å«æ­¤é¡åˆ¥")
    
    # â­ Step 2: ç‚ºæ¯å€‹é¡åˆ¥é¸æ“‡åœ–ç‰‡
    print(f"\nğŸ¯ å»ºç«‹å¹³è¡¡é›†...")
    
    selected_images = set()
    
    for class_id in sorted(class_image_map.keys()):
        class_images = list(set(class_image_map[class_id]))
        current_count = len(class_images)
        
        if current_count >= target_samples_per_class:
            # Under-sampling
            sampled = random.sample(class_images, target_samples_per_class)
            print(f"   Class {class_id}: Under-sample {current_count} â†’ {target_samples_per_class}")
        else:
            # Over-sampling (å…è¨±é‡è¤‡)
            repeat_times = target_samples_per_class // current_count
            remainder = target_samples_per_class % current_count
            
            sampled = class_images * repeat_times
            sampled += random.sample(class_images, remainder)
            
            print(f"   Class {class_id}: Over-sample {current_count} â†’ {target_samples_per_class} (é‡è¤‡ ~{repeat_times}x)")
        
        selected_images.update(sampled)
    
    # â­ Step 3: è¤‡è£½æª”æ¡ˆåˆ°æ–°ç›®éŒ„
    print(f"\nğŸ“¦ è¤‡è£½æª”æ¡ˆ...")
    
    copied_count = 0
    for img_name in selected_images:
        # æ‰¾åˆ°å°æ‡‰çš„åœ–ç‰‡æª”æ¡ˆ
        img_file = None
        for ext in ['.jpg', '.png', '.jpeg']:
            candidate = original_train_img / f"{img_name}{ext}"
            if candidate.exists():
                img_file = candidate
                break
        
        if img_file is None:
            print(f"âš ï¸  æ‰¾ä¸åˆ°åœ–ç‰‡: {img_name}")
            continue
        
        label_file = original_train_label / f"{img_name}.txt"
        
        if not label_file.exists():
            print(f"âš ï¸  æ‰¾ä¸åˆ°æ¨™ç±¤: {img_name}.txt")
            continue
        
        # è¤‡è£½
        shutil.copy(img_file, output_img_dir / img_file.name)
        shutil.copy(label_file, output_label_dir / label_file.name)
        copied_count += 1
    
    print(f"\nâœ… å®Œæˆ! è¤‡è£½äº† {copied_count} å¼µåœ–ç‰‡")
    print(f"   è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # â­ Step 4: å‰µå»ºæ–°çš„ data.yaml
    create_balanced_yaml(output_dir)
    
    return output_dir


def create_balanced_yaml(balanced_dir):
    """å‰µå»ºæ–°çš„ data.yaml çµ¦å¹³è¡¡é›†"""
    
    yaml_content = {
        'path': str(Path.cwd()),
        'train': str(balanced_dir / 'images'),
        'val': 'data/val/images',  # â­ é©—è­‰é›†ä¸è®Š
        'test': 'data/test/images',
        'nc': 4,
        'names': {
            0: 'class_0',
            1: 'class_1',
            2: 'class_2',
            3: 'class_3'
        }
    }
    
    yaml_path = Path('data_balanced.yaml')
    
    with open(yaml_path, 'w') as f:
        yaml.dump(yaml_content, f, default_flow_style=False)
    
    print(f"\nğŸ“ å‰µå»ºæ–°é…ç½®: {yaml_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--target', type=int, default=3000,
                        help='æ¯å€‹é¡åˆ¥çš„ç›®æ¨™æ¨£æœ¬æ•¸')
    
    args = parser.parse_args()
    
    create_balanced_dataset(target_samples_per_class=args.target)